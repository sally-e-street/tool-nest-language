#############
### SETUP ###
#############

# clear workspace
rm(list=ls())

# load packages 
packages<-c("data.table", "tidytext", "SnowballC", "cld3", "pscl") # list packages
install.packages(packages[!packages %in% rownames(installed.packages())]) # install packages if required
invisible(lapply(packages, library, character.only=TRUE)) # load packages

# turn off scientific notation
options(scipen=999)

#################
### DATA PREP ###
#################

# load data from WoS searches
tool_data<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/WoS_data/all_tool_results.csv") # tool use (all species): 1702 articles
nest_data<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/WoS_data/all_nest_results.csv") # nest building (all species): 6132 articles
ape_tool_data<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/WoS_data/ape_tool_results.csv") # tool use in apes: 578 articles
ape_nest_data<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/WoS_data/ape_nest_results.csv") # nest building in apes: 74 articles
corvus_tool_data<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/WoS_data/corvus_tool_results.csv") # tool use in corvus: 106 articles
corvus_nest_data<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/WoS_data/corvus_nest_results.csv") # nest building in corvus: 87 articles

sapply(list(tool_data, nest_data, ape_tool_data, ape_nest_data, corvus_tool_data, corvus_nest_data), nrow) # check number of rows in each file

# add labels for behaviour to prepare for combining tool and nest datasets
tool_data$Behaviour<-"Tool"
nest_data$Behaviour<-"Nest"

# check if any articles are included both tool and nest subsets
sum(tool_data$Article.Title %in% nest_data$Article.Title) # tool use articles in nest data: 28 
sum(nest_data$Article.Title %in% tool_data$Article.Title) # nest building articles in tool data: 28

# check and remove duplicated articles as appropriate 
tool_nest_dupes<-subset(tool_data, tool_data$Article.Title %in% nest_data$Article.Title)$Article.Title # list articles in both datasets for manual checking
nest_dupes<-tool_nest_dupes[c(1, 3, 5, 6, 10, 13)] # articles primarily relevant to tool use, to remove from nest database
tool_dupes<-tool_nest_dupes[c(7, 11, 15, 16, 17, 19, 20, 21, 22, 23, 24, 25, 26, 27)] # articles primarily relevant to nest building, to remove from tool database - all other articles relevant to both
nest_data<-subset(nest_data, !(Article.Title %in% nest_dupes)) # remove tool articles from nest data
nrow(nest_data) # 6126 articles remaining
tool_data<-subset(tool_data, !(Article.Title %in% tool_dupes)) # remove nest articles from tool data
nrow(tool_data) # 1688 articles remaining

# combine nest and tool data into a single object
nest_data$Include<-NA # add extra column so rbind can be used to combine files - this column is present only in the tool use dataset because manual checking for irrelevant articles was not necessary for nest searches
tool_data<-tool_data[,c("Include", "Publication.Type", "Article.Title", "Publication.Year", "Abstract", "Source.Title", "Times.Cited..All.Databases", "Behaviour")] # select columns of interest
nest_data<-nest_data[,c("Include", "Publication.Type", "Article.Title", "Publication.Year", "Abstract", "Source.Title", "Times.Cited..All.Databases", "Behaviour")]
colnames(tool_data)==colnames(nest_data) # double check all columns match before using rbind
combined_data<-rbind(tool_data, nest_data) # combine the data
nrow(combined_data) # 7814 articles total
table(combined_data$Behaviour) # 6126 nest articles, 1688 tool articles

# check for and remove any articles duplicated within each behaviour type
sum(duplicated(paste(combined_data$Article.Title, combined_data$Behaviour, sep=" "))) # 24 duplicates within each behaviour type
combined_data$Duplicated<-ifelse(duplicated(paste(combined_data$Article.Title, combined_data$Behaviour, sep=" ")), 1, 0) # create new column indicating which articles are duplicated
combined_data<-subset(combined_data, Duplicated==0) # remove the duplicates
nrow(combined_data) # 7790 articles remaining
table(combined_data$Behaviour) # 6108 nest building articles, 1682 tool use articles

# add categories for ape and corvus articles (after removing exclusions)
ape_tool_data<-subset(ape_tool_data, Include==1) # remove excluded articles
ape_nest_data<-subset(ape_nest_data, Include==1)
corvus_nest_data<-subset(corvus_nest_data, Include==1)
corvus_tool_data<-subset(corvus_tool_data, Include==1)

combined_data$Ape<-ifelse(combined_data$Article.Title %in% ape_tool_data$Article.Title | combined_data$Article.Title %in% ape_nest_data$Article.Title, "Y", "N") # create new column for ape (Y=ape, N=not ape)

combined_data$Corvus<-ifelse(combined_data$Article.Title %in% corvus_tool_data$Article.Title | combined_data$Article.Title %in% corvus_nest_data$Article.Title, "Y", "N") # create new column for Corvus (Y=Corvus, N=not Corvus)

table(combined_data$Ape) # 640 ape articles
table(combined_data$Corvus) # 179 Corvus articles

table(combined_data$Ape, combined_data$Behaviour) # 74 nest articles, 566 tool articles for apes
table(combined_data$Corvus, combined_data$Behaviour) # 75 nest articles, 104 tool articles for Corvus

# check for and remove excluded articles in the main datasets
sum(combined_data$Include==0, na.rm=T) # 140 articles to remove due to irrelevance 
table(combined_data$Include, combined_data$Behaviour) # all articles to exclude are from the tool use dataset
combined_data<-subset(combined_data, Include==1 | is.na(Include)) 
nrow(combined_data) # 7650 articles remaining
table(combined_data$Behaviour) # 6108 nest building articles, 1542 tool use articles
table(combined_data$Behaviour, combined_data$Ape) # 74 nest building articles, 566 tool use articles for apes
table(combined_data$Behaviour, combined_data$Corvus) # 75 nest building articles, 104 tool use articles for Corvus

# add journal impact factors and subject categories
journals<-unique(subset(combined_data, Publication.Type=="J")$Source.Title) # unique journal titles, selecting only articles with type J i.e. journal
length(journals) # 1823 unique journal names included in the searches
journals<-as.data.frame(journals) # convert to data frame
colnames(journals)<-"Name" # change column name

# import impact factors and subject category data from searches of Clarivate's  JCR tool (note: multiple searches had to be run for different categories, because you can only export 600 results at a time)
JCR_anthro<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_anthro.csv") # anthropology journals
JCR_arch<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_arch.csv") # archaeology journals
JCR_beh_sci<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_beh_sci.csv") # behavioural science journals
JCR_biology<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_biology.csv") # biology journals
JCR_biodiv<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_biodiv.csv") # biodiversity journals
JCR_ecology<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_ecology.csv") # ecology journals
JCR_ento<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_ento.csv") # entomology journals
JCR_env_sci<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_env_sci.csv") # environmental sciences journals
JCR_evo_devo<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_evo_devo.csv") # evolutionary and developmental biology journals
JCR_fish<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_fish.csv") # fisheries journals
JCR_multi<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_multi.csv") # multidisciplinary journals
JCR_orni<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_orni.csv") # ornithology journals
JCR_palaeo<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_palaeo.csv") # palaeontology journals
JCR_psych<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_psych.csv") # psychology journals
JCR_top600<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_top600.csv") # top 600 journals by IF
JCR_toxi<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_toxi.csv") # toxicology journals
JCR_vet<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_vet.csv") # veterinary sciences journals
JCR_zoology<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/JCR_zoology.csv") # zoology journals

JCR_journals<-rbindlist(list(JCR_anthro, JCR_arch, JCR_beh_sci, JCR_biology, JCR_biodiv, JCR_ecology, JCR_ento, JCR_env_sci, JCR_evo_devo, JCR_fish, JCR_multi, JCR_orni, JCR_palaeo, JCR_psych, JCR_top600, JCR_toxi, JCR_vet,  JCR_zoology)) # combine JCR search results into a single dataframe

length(JCR_journals$Journal.name) # 3968 JCR search results
length(unique(JCR_journals$Journal.name)) # 3004 unique journal names - some are duplicates due to journals belonging to multiple categories and appearing in multiple lists

journals_info<-merge(journals, JCR_journals[,c("Category", "X2023.JIF", "Journal.name")], by.x="Name", by.y="Journal.name", all.x=TRUE) # merge info from JCR searches with list of journals in the nest and tool data

sum(!is.na(journals_info$X2023.JIF)) # only 108 journals could be automatically matched

# merge journal info with data after manual data collection and subject categorisation
journals_manual<-read.csv("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/JCR_data/journals_manual.csv")
str(journals_manual) # all 1823 journals now checked
sum(!is.na(journals_manual$X2023.JIF)) # info available for 595 journals 
length(unique(journals_manual$Name)) # no duplicates remaining

# add journal information to the combined data
combined_data<-merge(combined_data, journals_manual, by.x="Source.Title", by.y="Name", all.x=TRUE) 
nrow(combined_data) # 7650 articles
sum(combined_data$Publication.Type=="J") # of which 7360 are journal articles
sum(!is.na(combined_data$X2023.JIF)) # JCR info available for 4277 articles
table(subset(combined_data, !is.na(X2023.JIF))$Behaviour) # JCR info available for 2948 nest building articles and 1329 tool use articles
table(subset(combined_data, !is.na(X2023.JIF))$Behaviour, subset(combined_data, !is.na(X2023.JIF))$Ape) # JCR info available for 50 nest building articles and 475 tool use articles for apes
table(subset(combined_data, !is.na(X2023.JIF))$Behaviour, subset(combined_data, !is.na(X2023.JIF))$Corvus) # JCR info available for 15 nest building articles and 97 tool use articles for Corvus

# create subsets of data including only articles with abstracts
combined_data_abstracts<-subset(combined_data, Abstract!="") 
nrow(combined_data_abstracts) # 4111 articles with abstracts available
table(combined_data_abstracts$Behaviour) # 2915 nest building and 1196 tool use articles with abstracts
table(combined_data_abstracts$Behaviour, combined_data_abstracts$Ape) # 48 nest building and 430 tool use articles with abstracts for apes
table(combined_data_abstracts$Behaviour, combined_data_abstracts$Corvus) # 28 nest building and 87 tool use articles with abstracts for Corvus

# check for non-English language abstracts
combined_data_abstracts$English<-ifelse(detect_language(combined_data_abstracts$Abstract)=="en", 1, 0)
table(combined_data_abstracts$English, useNA="ifany") # 1 not English, 3 NA
subset(combined_data_abstracts, English==0)$Abstract # 1 in Spanish and English
subset(combined_data_abstracts, is.na(English))$Abstract # NA are all actually in English, likely misclassified due to use of Latin names/non-English place names
combined_data_abstracts<-subset(combined_data_abstracts, English==1 | is.na(English)) 
str(combined_data_abstracts) # 4110 articles remaining

# add % 'intelligent' words in each article's abstract
dictionary<-read.table("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/Dictionaries/custom_dictionary.txt", header=T, sep="\t") # load dictionary (includes the list of 'intelligent' words for matching)

# convert dictionary words to stems
dictionary_stems <- wordStem(dictionary$word,  language = "english")
length(unique(dictionary_stems)) # check for duplicates

# calculate percentage of 'intelligent' words in each article abstract (i.e. N matches per abstract as a % of total words in each abstract)
combined_data_abstracts$Intel_percent<-NA # create a blank column for results
for (i in 1:length(combined_data_abstracts$Intel_percent)){ # for each article
	tokens<-unnest_tokens(tbl=combined_data_abstracts[i,], output=word, input=Abstract, token="words", to_lower=TRUE)$word # tokenize abstracts by word (i.e. split into individual words)
	stems<-wordStem(tokens,  language = "english") # convert words to stems
	combined_data_abstracts$Intel_percent[i]<-length(which(stems %in% dictionary_stems))/length(stems)*100
	}
	
# repeat using alternative dictionary (just intelligent + 20 synonyms in Collins thesaurus)
alt_dictionary<-read.table("https://raw.githubusercontent.com/sally-e-street/tool-nest-language/refs/heads/main/Dictionaries/alt_dictionary.txt", header=F, sep="\t") # load alternative dictionary
length(alt_dictionary$V1) # 21 words

alt_dictionary_stems <- wordStem(alt_dictionary$V1,  language = "english") # convert dictionary words to stems
length(unique(alt_dictionary_stems)) # no duplicates

# calculate percentage of matching 'intelligent' words in each article abstract (i.e. N matches per abstract as a % of total words in each abstract)
combined_data_abstracts$Alt_percent<-NA # blank column for results
for (i in 1:length(combined_data_abstracts$Alt_percent)){
	tokens<-unnest_tokens(tbl= combined_data_abstracts[i,], output=word, input=Abstract, token="words", to_lower=TRUE)$word
	stems<-wordStem(tokens,  language = "english")
	combined_data_abstracts$Alt_percent[i]<-length(which(stems %in% alt_dictionary_stems))/length(stems)*100
	}

# descriptives and exploratory plots
par(mfrow=c(1,2))
hist(combined_data$Times.Cited..All.Databases, main="", breaks=50, xlab="Citations", las=1) # right-skewed
hist(combined_data$X2023.JIF, main="", breaks=50, xlab="Impact factor", las=1) # right-skewed

par(mfrow=c(1,2))
hist(log10(combined_data$Times.Cited..All.Databases+1), main="", breaks=50, xlab="Citations", las=1) # still skewed after log10 transformation
hist(log10(combined_data$X2023.JIF+1), main="", breaks=50, xlab="Impact factor", las=1) # some skew after log10 transformation

par(mfrow=c(1,2))
hist(combined_data_abstracts$Intel_percent, main="", breaks=30, xlab="% 'intelligent' terms", las=1) # right-skewed
hist(combined_data_abstracts$Alt_percent, main="", breaks=30, xlab="% alt 'intelligent' terms", las=1) # right-skewed/bimodal

par(mfrow=c(1,2))
hist(log10(combined_data_abstracts$Intel_percent+1), main="", breaks=30, xlab="% 'intelligent' terms", las=1) # still right-skewed after log transformation
hist(log10(combined_data_abstracts$Alt_percent+1), main="", breaks=30, xlab="% alt 'intelligent' terms", las=1) # still right-skewed/bimodal after log transformation

par(mfrow=c(1,1))
plot(combined_data_abstracts[,c("Times.Cited..All.Databases", "X2023.JIF", "Intel_percent", "Alt_percent")])

par(mfrow=c(1,1))
plot(log10(combined_data_abstracts[,c("Times.Cited..All.Databases", "X2023.JIF", "Intel_percent", "Alt_percent")]+1))


######################
### CITATION RATES ###
######################

# check for missing data
sum(is.na(combined_data$Times.Cited..All.Databases))

# descriptive statistics 
aggregate(Times.Cited..All.Databases~Behaviour, combined_data, FUN=length) 
aggregate(Times.Cited..All.Databases~Behaviour, combined_data, FUN=quantile)

# wilcox test
wilcox.test(Times.Cited..All.Databases~Behaviour, combined_data)

# repeat for apes
aggregate(Times.Cited..All.Databases~Behaviour, subset(combined_data, Ape=="Y"), FUN=length) 
aggregate(Times.Cited..All.Databases~Behaviour, subset(combined_data, Ape=="Y"), FUN=quantile)
wilcox.test(Times.Cited..All.Databases~Behaviour, subset(combined_data, Ape=="Y"))

# repeat for Corvus
aggregate(Times.Cited..All.Databases~Behaviour, subset(combined_data, Corvus =="Y"), FUN=length)
aggregate(Times.Cited..All.Databases~Behaviour, subset(combined_data, Corvus =="Y"), FUN=quantile)
wilcox.test(Times.Cited..All.Databases~Behaviour, subset(combined_data, Corvus=="Y"))

# compare tool use in apes vs. Corvus
wilcox.test(subset(combined_data, Ape=="Y" & Behaviour=="Tool")$Times.Cited..All.Databases, subset(combined_data, Corvus=="Y" & Behaviour=="Tool")$Times.Cited..All.Databases)

# compare nest building in apes vs. Corvus
wilcox.test(subset(combined_data, Ape=="Y" & Behaviour=="Nest")$Times.Cited..All.Databases, subset(combined_data, Corvus=="Y" & Behaviour=="Nest")$Times.Cited..All.Databases)

# repeat for cognition journals
aggregate(Times.Cited..All.Databases~Behaviour, subset(combined_data, Group=="Cognition"), FUN=length) 
aggregate(Times.Cited..All.Databases~Behaviour, subset(combined_data, Group=="Cognition"), FUN=quantile)
wilcox.test(Times.Cited..All.Databases~Behaviour, subset(combined_data, Group=="Cognition"))

# repeat for general interest journals
aggregate(Times.Cited..All.Databases~Behaviour, subset(combined_data, Group=="General"), FUN=length) 
aggregate(Times.Cited..All.Databases~Behaviour, subset(combined_data, Group=="General"), FUN=quantile)
wilcox.test(Times.Cited..All.Databases~Behaviour, subset(combined_data, Group=="General"))

################
### JOURNALS ###
################

# check for missing data
sum(is.na(combined_data$X2023.JIF)) # 3373 articles without journal info

# descriptive statistics 
aggregate(X2023.JIF~Behaviour, combined_data, FUN=length) 
aggregate(X2023.JIF~Behaviour, combined_data, FUN=quantile)

# wilcox test
wilcox.test(X2023.JIF~Behaviour, combined_data)

# repeat for apes
aggregate(X2023.JIF~Behaviour, subset(combined_data, Ape=="Y"), FUN=length) 
aggregate(X2023.JIF~Behaviour, subset(combined_data, Ape=="Y"), FUN=quantile)
wilcox.test(X2023.JIF~Behaviour, subset(combined_data, Ape=="Y"))

# repeat for Corvus
aggregate(X2023.JIF~Behaviour, subset(combined_data, Corvus =="Y"), FUN=length)
aggregate(X2023.JIF~Behaviour, subset(combined_data, Corvus =="Y"), FUN=quantile)
wilcox.test(X2023.JIF~Behaviour, subset(combined_data, Corvus=="Y"))

# compare tool use in apes vs. Corvus
wilcox.test(subset(combined_data, Ape=="Y" & Behaviour=="Tool")$X2023.JIF, subset(combined_data, Corvus=="Y" & Behaviour=="Tool")$X2023.JIF)

# compare nest building in apes vs. Corvus
wilcox.test(subset(combined_data, Ape=="Y" & Behaviour=="Nest")$X2023.JIF, subset(combined_data, Corvus=="Y" & Behaviour=="Nest")$X2023.JIF)

# repeat for cognition journals
aggregate(X2023.JIF~Behaviour, subset(combined_data, Group=="Cognition"), FUN=length) 
aggregate(X2023.JIF~Behaviour, subset(combined_data, Group=="Cognition"), FUN=quantile)
wilcox.test(X2023.JIF~Behaviour, subset(combined_data, Group=="Cognition"))

# repeat for general interest journals
aggregate(X2023.JIF~Behaviour, subset(combined_data, Group=="General"), FUN=length) 
aggregate(X2023.JIF~Behaviour, subset(combined_data, Group=="General"), FUN=quantile)
wilcox.test(X2023.JIF~Behaviour, subset(combined_data, Group=="General"))

# frequency table for journal categories
table(combined_data$Group, combined_data$Behaviour, useNA="ifany")

# prop table for journal categories (converted to percentages across rows)
prop.table(table(combined_data$Group, combined_data$Behaviour, useNA="ifany"), 1)*100

# chi-square test: general interest/not general interest vs. nests/tools
combined_data$General<-ifelse(combined_data$Group=="General", "Y", "N")
table(combined_data$General, combined_data$Behaviour, useNA="ifany")
prop.table(table(combined_data$General, combined_data$Behaviour), 2)*100
chisq.test(table(combined_data$General, combined_data$Behaviour))

# chi-square test: cognition-focused/not cognition-focused vs. nests/tools
combined_data$Cognition<-ifelse(combined_data$Group=="Cognition", "Y", "N")
table(combined_data$Cognition, combined_data$Behaviour, useNA="ifany")
prop.table(table(combined_data$Cognition, combined_data$Behaviour), 2)*100
chisq.test(table(combined_data$Cognition, combined_data$Behaviour))

# chi-square test: human-focused/not human-focused vs. nests/tools
combined_data$Anthro<-ifelse(combined_data$Group=="Anthro", "Y", "N")
table(combined_data$Anthro, combined_data$Behaviour, useNA="ifany")
prop.table(table(combined_data$Anthro, combined_data$Behaviour), 2)*100
chisq.test(table(combined_data$Anthro, combined_data$Behaviour))

# chi-square test: ecology-focused/not ecology-focused vs. nests/tools
combined_data$Ecology<-ifelse(combined_data$Group=="Ecology", "Y", "N")
table(combined_data$Ecology, combined_data$Behaviour, useNA="ifany")
prop.table(table(combined_data$Ecology, combined_data$Behaviour), 2)*100
chisq.test(table(combined_data$Ecology, combined_data$Behaviour))

# chi-square test: taxon-specific/not taxon-specific vs. nests/tools
combined_data$Taxon<-ifelse(combined_data$Group=="Taxon", "Y", "N")
table(combined_data$Taxon, combined_data$Behaviour, useNA="ifany")
prop.table(table(combined_data$Taxon, combined_data$Behaviour), 2)*100
chisq.test(table(combined_data$Taxon, combined_data$Behaviour))

# chi-square test: applied focus/not applied focus vs. nests/tools
combined_data$Applied<-ifelse(combined_data$Group=="Applied", "Y", "N")
table(combined_data$Applied, combined_data$Behaviour, useNA="ifany")
prop.table(table(combined_data$Applied, combined_data$Behaviour), 2)*100
chisq.test(table(combined_data$Applied, combined_data$Behaviour))

#########################
### ABSTRACT LANGUAGE ###
#########################

# check for missing data
sum(is.na(combined_data_abstracts$Intel_percent)) 

# descriptive statistics 
aggregate(Intel_percent~Behaviour, combined_data_abstracts, FUN=length) 
aggregate(Intel_percent~Behaviour, combined_data_abstracts, FUN=quantile)

# wilcox test
wilcox.test(Intel_percent~Behaviour, combined_data_abstracts)

# repeat for alternative dictionary
aggregate(Alt_percent~Behaviour, combined_data_abstracts, FUN=length) 
aggregate(Alt_percent~Behaviour, combined_data_abstracts, FUN=quantile)
wilcox.test(Alt_percent~Behaviour, combined_data_abstracts)

# repeat for apes
aggregate(Intel_percent~Behaviour, subset(combined_data_abstracts, Ape=="Y"), FUN=length) 
aggregate(Intel_percent~Behaviour, subset(combined_data_abstracts, Ape=="Y"), FUN=quantile)
wilcox.test(Intel_percent~Behaviour, subset(combined_data_abstracts, Ape=="Y"))

# repeat for Corvus
aggregate(Intel_percent~Behaviour, subset(combined_data_abstracts, Corvus =="Y"), FUN=length)
aggregate(Intel_percent~Behaviour, subset(combined_data_abstracts, Corvus =="Y"), FUN=quantile)
wilcox.test(Intel_percent~Behaviour, subset(combined_data_abstracts, Corvus=="Y"))

# compare tool use in apes vs. Corvus
wilcox.test(subset(combined_data_abstracts, Ape=="Y" & Behaviour=="Tool")$Intel_percent, subset(combined_data_abstracts, Corvus=="Y" & Behaviour=="Tool")$Intel_percent)

# compare nest building in apes vs. Corvus
wilcox.test(subset(combined_data_abstracts, Ape=="Y" & Behaviour=="Nest")$Intel_percent, subset(combined_data_abstracts, Corvus=="Y" & Behaviour=="Nest")$Intel_percent)

# repeat for cognition journals
aggregate(Intel_percent~Behaviour, subset(combined_data_abstracts, Group=="Cognition"), FUN=length) 
aggregate(Intel_percent~Behaviour, subset(combined_data_abstracts, Group=="Cognition"), FUN=quantile)
wilcox.test(Intel_percent~Behaviour, subset(combined_data_abstracts, Group=="Cognition"))

# repeat for general interest journals
aggregate(Intel_percent~Behaviour, subset(combined_data_abstracts, Group=="General"), FUN=length) 
aggregate(Intel_percent~Behaviour, subset(combined_data_abstracts, Group=="General"), FUN=quantile)
wilcox.test(Intel_percent~Behaviour, subset(combined_data_abstracts, Group=="General"))

# frequencies of individual stems in tool use vs. nest building papers
tool_tokens<-unnest_tokens(tbl=subset(combined_data_abstracts, Behaviour=="Tool"), output=word, input=Abstract, token="words", to_lower=TRUE)$word # extract words for tool use papers
length(tool_tokens) # total number of words in tool use abstracts
nest_tokens<-unnest_tokens(tbl=subset(combined_data_abstracts, Behaviour=="Nest"), output=word, input=Abstract, token="words", to_lower=TRUE)$word # extract words for nest buidling papers
length(nest_tokens) # total number of words in nest building abstracts

tool_stems<-wordStem(tool_tokens,  language = "english") # convert tool use abstract words to stems
nest_stems<-wordStem(nest_tokens,  language = "english") # convert nest building abstract words to stems

toolmatch_stems<-as.data.frame(table(tool_stems[which(tool_stems %in% dictionary_stems)])) # frequency table for each matching stem for tool use
colnames(toolmatch_stems)<-c("Stem", "Tool_freq") # change column names for convenience
nestmatch_stems<-as.data.frame(table(nest_stems[which(nest_stems %in% dictionary_stems)])) # frequency table for each matching stem for nest building
colnames(nestmatch_stems)<-c("Stem", "Nest_freq") # change column names for convenience

merged_match_stems<-merge(toolmatch_stems, nestmatch_stems, by="Stem", all=TRUE) # merge the matching stems
merged_match_stems[is.na(merged_match_stems)]<-0 # replace NAs with zeros (so we can include stems found in one dataset but not the other)

merged_match_stems$Tool_percent<-merged_match_stems$Tool_freq/length(tool_stems)*100 # calculate percentage matches for each matching stem for tool use, out of the total number of words in the tool use abstracts 
merged_match_stems$Nest_percent<-merged_match_stems$Nest_freq/length(nest_stems)*100 # calculate percentage matches for each matching stem for nest building, out of the total number of words in the nest building abstracts 

merged_match_stems$Percent_diff<-merged_match_stems$Tool_percent-merged_match_stems$Nest_percent # calculate absolute difference in % matches for tool use vs. nest building papers

merged_match_stems<-merged_match_stems[order(merged_match_stems$Percent_diff, decreasing=T),] # order stems by the difference in % matches between tool use and nest building papers

# repeat for alternative dictionary
toolmatch_alt_stems<-as.data.frame(table(tool_stems[which(tool_stems %in% alt_dictionary_stems)])) 
colnames(toolmatch_alt_stems)<-c("Stem", "Tool_freq")
nestmatch_alt_stems<-as.data.frame(table(nest_stems[which(nest_stems %in% alt_dictionary_stems)])) 
colnames(nestmatch_alt_stems)<-c("Stem", "Nest_freq")

merged_match_alt_stems<-merge(toolmatch_alt_stems, nestmatch_alt_stems, by="Stem", all=TRUE) 
merged_match_alt_stems[is.na(merged_match_alt_stems)]<-0 

merged_match_alt_stems$Tool_percent<-merged_match_alt_stems$Tool_freq/length(tool_stems)*100 
merged_match_alt_stems$Nest_percent<-merged_match_alt_stems$Nest_freq/length(nest_stems)*100

merged_match_alt_stems$Percent_diff<-merged_match_alt_stems$Tool_percent-merged_match_alt_stems$Nest_percent 

merged_match_alt_stems<-merged_match_alt_stems[order(merged_match_alt_stems$Percent_diff, decreasing=T),] 


# FIGURE 1 a-c
dev.off() 
dev.new(height=3.5, width=10.5, unit="in") # set plot size
par(mfrow=c(1,3)) # box plots for citations, impact factors and % 'intelligent' terms in abstracts
boxplot(log10(Times.Cited..All.Databases+1)~Behaviour, combined_data, col=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)), border=c(rgb(0,0,1,0.75), rgb(1,0,0,0.75)), las=1, yaxt="n", xlab="", ylab="Citations", range=1.5, names=c("Nests", "Tools"), cex.lab=1.2, cex.axis=1.2)
axis(side=2, at=log10(seq(from=1, to=1001, 10)), labels=rep("", 101)) # set custom axes appropriate for log10(x+1) transformed data
axis(side=2, at=c(0, log10(11),log10(101), log10(1001)), labels=c(0,10,100,1000), las=1)	

boxplot(log10(X2023.JIF+1)~Behaviour, combined_data, col=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)), border=c(rgb(0,0,1,0.75), rgb(1,0,0,0.75)), las=1, yaxt="n", xlab="", ylab="Impact factor", range=1.5, names=c("Nests", "Tools"), cex.lab=1.2, cex.axis=1.2)
axis(side=2, at=log10(1:51), labels=rep("", 51))
axis(side=2, at=c(0, log10(2),log10(3), log10(4),log10(6), log10(11), log10(21), log10(51)), labels=c(0,1,2,"",5,10,20,50), las=1)	

boxplot(log10(Intel_percent+1)~Behaviour, combined_data_abstracts, col=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)), border=c(rgb(0,0,1,0.75), rgb(1,0,0,0.75)), las=1,  xlab="", yaxt="n", ylab="% 'intelligent' terms", range=1.5, names=c("Nests", "Tools"), cex.lab=1.2, cex.axis=1.2)
axis(side=2, at=log10(1:14), labels=rep("", 14))
axis(side=2, at=c(0, log10(2), log10(3), log10(6), log10(11)), labels=c(0, 1, 2, 5, 10), las=1)	

# FIGURE 1 d
dev.off()
dev.new(height=4, width=10.5, unit="in")
par(mfrow=(c(1,1))) # barplot for proportion of nest vs. tool use papers in each journal subject category
combined_data$Group<-factor(combined_data$Group, levels=c("Ecology", "Applied", "Taxon", "Other_bio", "Zoology", "General", "Cognition", "Anthro")) # re-order groups for illustrative purposes
xpos_bp_journals<-barplot(prop.table(table(combined_data$Behaviour, combined_data$Group), 2), border=c("blue", "red"), col=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)), ylab="Proportion", xlab="", las=1, cex.axis=0.8, cex.lab=0.8, cex.names=0.8, names.arg=c("Ecology", "Applied", "Taxon", "Other bio.", "Zoology", "General", "Cognition", "Anthro"))
abline(h=prop.table(table(subset(combined_data, !is.na(Group))$Behaviour))[1], lty=2) # overall proportion of nest building papers
text(x=xpos_bp_journals, y=0.03, table(combined_data$Behaviour, combined_data$Group)[1,], col="blue", cex=0.8) # add frequencies for nest building papers
text(x=xpos_bp_journals, y=0.97, table(combined_data$Behaviour, combined_data$Group)[2,], col="red", cex=0.8) # add frequencies for tool use papers

# FIGURE 1 e
dev.off()
dev.new(height=3.5, width=10.5, unit="in")
par(mfrow=(c(1,1))) # barplot showing % matching stems for nest building and tool use papers
x<-barplot(height=merged_match_stems$Tool_percent, names=merged_match_stems$Stem, xaxt="n", cex.names=0.8, las=2, cex.axis=0.8, col=rgb(1,0,0,0.5), border="red", xlab="Stem", ylab="% matches", cex.lab=0.8)
text(cex=0.8, x=x, y=-0.03, merged_match_stems$Stem, xpd=T, srt=45, adj=1)
barplot(height=merged_match_stems$Nest_percent, add=T, col=rgb(0,0,1,0.5), border="blue", axes=F, yaxt="n", xaxt="n")
legend("topright", legend=c("Nests", "Tools"), pch=22, pt.bg=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)), col=c("blue", "red"), cex=0.8)	

# FIGURE S2 
dev.off()
dev.new(height=3.5, width=10.5, unit="in")
par(mfrow=(c(1,1))) # barplot showing % matching stems for nest building and tool use papers, using alternative dictionary
x<-barplot(height=merged_match_alt_stems$Tool_percent, names=merged_match_alt_stems$Stem, xaxt="n", cex.names=0.8, las=2, cex.axis=0.8, col=rgb(1,0,0,0.5), border="red", xlab="Stem", ylab="% matches", cex.lab=0.8)
text(cex=0.8, x=x, y=-0.005, merged_match_alt_stems$Stem, xpd=T, srt=45, adj=1)
barplot(height=merged_match_alt_stems$Nest_percent, add=T, col=rgb(0,0,1,0.5), border="blue", axes=F, yaxt="n", xaxt="n")
legend("topright", legend=c("Nests", "Tools"), pch=22, pt.bg=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)), col=c("blue", "red"), cex=0.8)	

# FIGURE S3 a,c,e
dev.off()
dev.new(height=10.5, width=3.5, unit="in")
par(mfrow=c(3,1)) # boxplots for citations, impact factors and % 'intelligent' terms for ape species only
boxplot(log10(Times.Cited..All.Databases+1)~Behaviour, subset(combined_data, Ape=="Y"), col=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)), border=c(rgb(0,0,1,0.75), rgb(1,0,0,0.75)), las=1, yaxt="n", xlab="", ylab="Citations", range=1.5, names=c("Nests", "Tools"), cex.lab=1.2, cex.axis=1.2)
axis(side=2, at=log10(seq(from=1, to=1001, 10)), labels=rep("", 101))
axis(side=2, at=c(0, log10(11),log10(101), log10(1001)), labels=c(0,10,100,1000), las=1)	

boxplot(log10(X2023.JIF+1)~Behaviour, subset(combined_data, Ape=="Y"), col=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)), border=c(rgb(0,0,1,0.75), rgb(1,0,0,0.75)), las=1, yaxt="n", xlab="", ylab="Impact factor", range=1.5, names=c("Nests", "Tools"), cex.lab=1.2, cex.axis=1.2)
axis(side=2, at=log10(1:51), labels=rep("", 51))
axis(side=2, at=c(0, log10(2),log10(3), log10(4),log10(6), log10(11), log10(21), log10(51)), labels=c(0,1,2,"",5,10,20,50), las=1)	

boxplot(log10(Intel_percent+1)~Behaviour, subset(combined_data_abstracts, Ape=="Y"), col=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)), border=c(rgb(0,0,1,0.75), rgb(1,0,0,0.75)), las=1,  xlab="", yaxt="n", ylab="% 'intelligent' terms", range=1.5, names=c("Nests", "Tools"), cex.lab=1.2, cex.axis=1.2)
axis(side=2, at=log10(1:14), labels=rep("", 14))
axis(side=2, at=c(0, log10(2), log10(3), log10(6), log10(11)), labels=c(0, 1, 2, 5, 10), las=1)	

# FIGURE S3 b,d,f
dev.off()
dev.new(height=10.5, width=3.5, unit="in")
par(mfrow=c(3,1)) # boxplots for citations, impact factors and % 'intelligent' terms for Corvus species only
boxplot(log10(Times.Cited..All.Databases+1)~Behaviour, subset(combined_data, Corvus=="Y"), col=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)), border=c(rgb(0,0,1,0.75), rgb(1,0,0,0.75)), las=1, yaxt="n", xlab="", ylab="Citations", range=1.5, names=c("Nests", "Tools"), cex.lab=1.2, cex.axis=1.2)
axis(side=2, at=log10(seq(from=1, to=1001, 10)), labels=rep("", 101))
axis(side=2, at=c(0, log10(11),log10(101), log10(1001)), labels=c(0,10,100,1000), las=1)	

boxplot(log10(X2023.JIF+1)~Behaviour, subset(combined_data, Corvus=="Y"), col=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)), border=c(rgb(0,0,1,0.75), rgb(1,0,0,0.75)), las=1, yaxt="n", xlab="", ylab="Impact factor", range=1.5, names=c("Nests", "Tools"), cex.lab=1.2, cex.axis=1.2)
axis(side=2, at=log10(1:51), labels=rep("", 51))
axis(side=2, at=c(0, log10(2),log10(3), log10(4),log10(6), log10(11), log10(21), log10(51)), labels=c(0,1,2,"",5,10,20,50), las=1)	

boxplot(log10(Intel_percent+1)~Behaviour, subset(combined_data_abstracts, Corvus=="Y"), col=c(rgb(0,0,1,0.5), rgb(1,0,0,0.5)), border=c(rgb(0,0,1,0.75), rgb(1,0,0,0.75)), las=1,  xlab="", yaxt="n", ylab="% 'intelligent' terms", range=1.5, names=c("Nests", "Tools"), cex.lab=1.2, cex.axis=1.2, ylim=c(0, log10(11)))
axis(side=2, at=log10(1:14), labels=rep("", 14))
axis(side=2, at=c(0, log10(2), log10(3), log10(6), log10(11)), labels=c(0, 1, 2, 5, 10), las=1)	


#########################
### REGRESSION MODELS ###
#########################

# model comparison
summary(null_model<-hurdle(Times.Cited..All.Databases~1, combined_data_abstracts, dist="poisson", zero.dist="binomial", link="logit")) # null model, intercept only
summary(behaviour_model<-hurdle(Times.Cited..All.Databases~Behaviour, combined_data_abstracts, dist="poisson", zero.dist="binomial", link="logit")) # behaviour only (tools vs. nests)
summary(intel_model<-hurdle(Times.Cited..All.Databases~log10(Intel_percent+1), combined_data_abstracts, dist="poisson", zero.dist="binomial", link="logit")) # language only (% 'intelligent' terms in abstracts)
summary(full_model<-hurdle(Times.Cited..All.Databases~Behaviour+log10(Intel_percent+1), combined_data_abstracts, dist="poisson", zero.dist="binomial", link="logit")) # behaviour + language
summary(full_int_model<-hurdle(Times.Cited..All.Databases~Behaviour*log10(Intel_percent+1), combined_data_abstracts, dist="poisson", zero.dist="binomial", link="logit")) # behaviour * language

AIC(null_model, behaviour_model, intel_model, full_model, full_int_model) # full model with interaction best supported

# McFadden's pseudo R^2 for full model
1-(logLik(full_int_model)[1]/logLik(null_model)[1])

# FIGURE 2
dev.off()
dev.new(width=5, height=5)
par(mfrow=c(1,1)) # scatterplot and fit lines for full model
plot(Times.Cited..All.Databases~log10(Intel_percent+1), subset(combined_data_abstracts, Behaviour=="Tool"), pch=19, col=rgb(1,0,0,0.1), las=1, ylab="Citations", xlab="% 'intelligent' words (log10 scale)")
points(Times.Cited..All.Databases~log10(Intel_percent+1), subset(combined_data_abstracts, Behaviour=="Nest"), pch=19, col=rgb(0,0,1,0.1))
legend("topleft", pch=19, legend=c("Tool use", "Nest building"), col=c(rgb(1,0,0,0.5), rgb(0,0,1,0.5)))

combined_data_abstracts$Fitted<-full_int_model$fitted.values # extract fitted values from full model with interaction

tool_spline<-smooth.spline(x=log10(subset(combined_data_abstracts, Behaviour=="Tool")$Intel_percent+1), y=subset(combined_data_abstracts, Behaviour=="Tool")$Fitted) # create a smoothed spline for tool use
nest_spline<-smooth.spline(x=log10(subset(combined_data_abstracts, Behaviour=="Nest")$Intel_percent+1), y=subset(combined_data_abstracts, Behaviour=="Nest")$Fitted) # create a smoothed spline for nest building

lines(tool_spline, col="red", lwd=2) # add lines to the plot
lines(nest_spline, col="blue", lwd=2)


########################
### CHANGE OVER TIME ###
########################

# descriptive statistics
range(combined_data_abstracts$Publication.Year)

# calculate percentage matches per year (i.e. matching stems of abstracts in a given year as a percentage of all stems in that year)
tool_words_year<-as.data.frame(cbind(unique(subset(combined_data_abstracts, Behaviour=="Tool")$Publication.Year), rep(NA, length(unique(subset(combined_data_abstracts, Behaviour=="Tool")$Publication.Year))))) # set up data for tool use
colnames(tool_words_year)<-c("Year", "Perc_matches") # change column names for convenience
tool_words_year<-tool_words_year[order(tool_words_year$Year),] # sort by year

for (i in 1:length(tool_words_year$Perc_matches)){ # calculate % matching stems for each year
	tokens<-unnest_tokens(tbl=subset(combined_data_abstracts, Behaviour=="Tool")[subset(combined_data_abstracts, Behaviour=="Tool")$Publication.Year==tool_words_year$Year[i],], output=word, input=Abstract, token="words", to_lower=TRUE)$word # extract words for year i
	stems<-wordStem(tokens,  language = "english") # convert to stems
	tool_words_year$Perc_matches[i]<-length(which(stems %in% dictionary_stems))/length(stems)*100 # % of matching stems of total stems for that year
	}
	
nest_words_year<-as.data.frame(cbind(unique(subset(combined_data_abstracts, Behaviour=="Nest")$Publication.Year), rep(NA, length(unique(subset(combined_data_abstracts, Behaviour=="Nest")$Publication.Year))))) # repeat for nests
colnames(nest_words_year)<-c("Year", "Perc_matches") 
nest_words_year<-nest_words_year[order(nest_words_year$Year),] 

for (i in 1:length(nest_words_year$Perc_matches)){
	tokens<-unnest_tokens(tbl=subset(combined_data_abstracts, Behaviour=="Nest")[subset(combined_data_abstracts, Behaviour=="Nest")$Publication.Year==nest_words_year$Year[i],], output=word, input=Abstract, token="words", to_lower=TRUE)$word
	stems<-wordStem(tokens,  language = "english")
	nest_words_year$Perc_matches[i]<-length(which(stems %in% dictionary_stems))/length(stems)*100
	}
	
merged_words_year<-merge(tool_words_year, nest_words_year, by="Year", all=TRUE) # merge tool and nest matches by year
colnames(merged_words_year)[2:3]<-c("Tool_matches", "Nest_matches") # rename columns 
merged_words_year[is.na(merged_words_year)]<-0 # change NAs to zeros
merged_words_year<-subset(merged_words_year, Year>1975) # remove articles prior to 1975 (only one article, from 1884)
merged_words_year<-subset(merged_words_year, Year<2024) # remove 2024 (incomplete year)

# linear models
toolmatches_year_lm<-lm(Tool_matches~Year, merged_words_year)
summary(toolmatches_year_lm)
qqnorm(toolmatches_year_lm$resid); qqline(toolmatches_year_lm$resid) # reasonable fit 

nestmatches_year_lm<-lm(Nest_matches~Year, merged_words_year)
summary(nestmatches_year_lm)
qqnorm(nestmatches_year_lm$resid); qqline(nestmatches_year_lm$resid) # good fit apart from one outlier

# FIGURE 3
dev.off()
dev.new(width=9, height=3)
par(mfrow=c(1,1)) # change in % of 'intelligent' words over time for nest building vs. tool use articles
plot(Tool_matches~Year, merged_words_year, pch=19, col=c(rgb(1,0,0,0.6)), las=1, ylab="% 'intelligent' terms", xaxt="n", cex=0.8, cex.lab=0.8, cex.axis=0.8)
abline(toolmatches_year_lm, col="red", lwd=2) # fit line for tools
points(Nest_matches~Year, merged_words_year, pch=19, col=c(rgb(0,0,1,0.6)), cex=0.8)
abline(nestmatches_year_lm, col="blue", lwd=2, cex=0.8) # fit line for nests
axis(side=1, at=c(seq(from=1975, to=2020, by=5)), cex.axis=0.8)
legend("topleft", legend=c("Tool use", "Nest building"), pch=19, cex=0.8, col=c(rgb(1,0,0,0.75), rgb(0,0,1,0.75)), bty='n')
